#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTM Price Direction Predictor
æ ¹æ®PRD_LSTM_Predictor.mdå®ç°çš„LSTMé¢„æµ‹ä¿¡å·ç”Ÿæˆå™¨
"""

import argparse
import json
import pickle
from pathlib import Path
from typing import Tuple, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import os


class FeatureEngine:
    """ç‰¹å¾å·¥ç¨‹ç±» - ç”Ÿæˆ8ä¸ªæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
    
    def __init__(self):
        self.feature_names = [
            'returns', 'log_returns', 'high_low_pct', 'close_position',
            'volume_change', 'returns_ma5', 'returns_ma20', 'volatility'
        ]
    
    def transform(self, df: pd.DataFrame) -> np.ndarray:
        """
        è®¡ç®—8ä¸ªç‰¹å¾
        
        Args:
            df: åŒ…å«OHLCVçš„DataFrame
            
        Returns:
            features: shape [N, 8]
        """
        df = df.copy()
        
        # 1. returns
        df['returns'] = df['close'].pct_change()
        
        # 2. log_returns
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        
        # 3. high_low_pct (æŒ¯å¹…)
        df['high_low_pct'] = (df['high'] - df['low']) / df['close']
        
        # 4. close_position (Kçº¿ä½ç½®)
        hl_range = df['high'] - df['low']
        df['close_position'] = (df['close'] - df['low']) / (hl_range + 1e-8)
        
        # 5. volume_change (é‡æ¯”)
        volume_ma20 = df['volume'].rolling(window=20, min_periods=1).mean()
        df['volume_change'] = df['volume'] / (volume_ma20 + 1e-8) - 1
        
        # 6. returns_ma5 (çŸ­æœŸåŠ¨é‡)
        df['returns_ma5'] = df['returns'].rolling(window=5, min_periods=1).mean()
        
        # 7. returns_ma20 (ä¸­æœŸåŠ¨é‡)
        df['returns_ma20'] = df['returns'].rolling(window=20, min_periods=1).mean()
        
        # 8. volatility (æ³¢åŠ¨ç‡)
        df['volatility'] = df['returns'].rolling(window=20, min_periods=1).std()
        
        # æå–ç‰¹å¾çŸ©é˜µ
        features = df[self.feature_names].values
        
        # å¤„ç†NaNå’ŒInf
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
        
        return features


class TargetEngine:
    """æ ‡ç­¾æ„é€ ç±»"""
    
    def __init__(self, horizon: int = 10, scale: float = 0.01):
        """
        Args:
            horizon: é¢„æµ‹æœªæ¥å¤šå°‘ä¸ªbar
            scale: tanhç¼©æ”¾å‚æ•°ï¼Œé»˜è®¤0.005 (0.5%)
        """
        self.horizon = horizon
        self.scale = scale
    
    def create_labels(self, df: pd.DataFrame) -> np.ndarray:
        """
        æ„é€ æ ‡ç­¾: tanh(future_return / scale)
        
        Args:
            df: åŒ…å«closeåˆ—çš„DataFrame
            
        Returns:
            labels: shape [N], å€¼åŸŸ[-1, +1]
        """
        close = df['close'].values
        
        # æœªæ¥æ”¶ç›Šç‡
        future_close = np.roll(close, -self.horizon)
        future_return = (future_close - close) / (close + 1e-8)
        
        # tanhæ˜ å°„åˆ°[-1, +1]
        labels = np.tanh(future_return / self.scale)
        
        # æœ€åhorizonä¸ªæ ·æœ¬æ²¡æœ‰æ ‡ç­¾ï¼Œè®¾ä¸º0
        labels[-self.horizon:] = 0.0
        
        return labels


class TimeSeriesDataset(Dataset):
    """æ—¶åºæ•°æ®é›†"""
    
    def __init__(self, features: np.ndarray, labels: np.ndarray, lookback: int):
        """
        Args:
            features: [N, F] ç‰¹å¾çŸ©é˜µ
            labels: [N] æ ‡ç­¾
            lookback: è¾“å…¥çª—å£é•¿åº¦
        """
        self.features = features
        self.labels = labels
        self.lookback = lookback
    
    def __len__(self):
        return len(self.features) - self.lookback + 1
    
    def __getitem__(self, idx):
        # å–lookbackä¸ªæ—¶é—´æ­¥
        x = self.features[idx:idx + self.lookback]
        y = self.labels[idx + self.lookback - 1]
        
        return torch.FloatTensor(x), torch.FloatTensor([y])


class LSTMModel(nn.Module):
    """LSTMé¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size: int = 8, hidden_size: int = 32, 
                 num_layers: int = 2, dropout: float = 0.2):
        super().__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LayerNorm
        self.layer_norm = nn.LayerNorm(input_size)
        
        # LSTM (å•å‘)
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=False
        )
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_size, 16)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(16, 1)
        self.tanh = nn.Tanh()
    
    def forward(self, x):
        """
        Args:
            x: [batch_size, lookback, input_size]
            
        Returns:
            output: [batch_size, 1], å€¼åŸŸ[-1, +1]
        """
        # LayerNorm
        x = self.layer_norm(x)
        
        # LSTM
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # å–æœ€åæ—¶é—´æ­¥çš„éšçŠ¶æ€
        last_hidden = h_n[-1]  # [batch_size, hidden_size]
        
        # å…¨è¿æ¥å±‚
        out = self.fc1(last_hidden)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        out = self.tanh(out)
        
        return out


class Trainer:
    """è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, device: str = 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.val_losses = []
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              epochs: int = 100, lr: float = 0.001, weight_decay: float = 0.0001,
              patience: int = 10) -> dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
            weight_decay: L2æ­£åˆ™åŒ–ç³»æ•°
            patience: æ—©åœè€å¿ƒå€¼
            
        Returns:
            training_log: è®­ç»ƒæ—¥å¿—
        """
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        print(f"\n{'='*50}")
        print(f"å¼€å§‹è®­ç»ƒ | Epochs: {epochs} | LR: {lr}")
        print(f"{'='*50}\n")
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            self.train_losses.append(train_loss)
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x = batch_x.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            self.val_losses.append(val_loss)
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"Epoch [{epoch+1:3d}/{epochs}] | "
                      f"Train Loss: {train_loss:.6f} | "
                      f"Val Loss: {val_loss:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                print(f"\næ—©åœè§¦å‘ï¼åœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
        
        training_log = {
            'epochs_trained': epoch + 1,
            'best_val_loss': best_val_loss,
            'final_train_loss': self.train_losses[-1],
            'train_losses': self.train_losses,
            'val_losses': self.val_losses
        }
        
        print(f"\n{'='*50}")
        print(f"è®­ç»ƒå®Œæˆ | æœ€ä½³éªŒè¯Loss: {best_val_loss:.6f}")
        print(f"{'='*50}\n")
        
        return training_log


class Predictor:
    """é¢„æµ‹å™¨ - ç”Ÿæˆä¿¡å·CSV"""
    
    def __init__(self, model: nn.Module, scaler: StandardScaler, 
                 lookback: int, device: str = 'cpu'):
        self.model = model.to(device)
        self.model.eval()
        self.scaler = scaler
        self.lookback = lookback
        self.device = device
    
    def predict(self, df: pd.DataFrame, feature_engine: FeatureEngine, batch_size: int = 256) -> pd.DataFrame:
        """
        ç”Ÿæˆé¢„æµ‹ä¿¡å·ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼Œæé«˜å¹¶å‘æ€§èƒ½ï¼‰
        
        Args:
            df: åŸå§‹OHLCVæ•°æ®
            feature_engine: ç‰¹å¾å·¥ç¨‹å¯¹è±¡
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            signal_df: åŒ…å«ts_eventå’Œsignalçš„DataFrame
        """
        # è®¡ç®—ç‰¹å¾
        features = feature_engine.transform(df)
        
        # æ ‡å‡†åŒ–
        features_scaled = self.scaler.transform(features)
        
        # åˆå§‹åŒ–ä¿¡å·æ•°ç»„
        signals = np.full(len(features_scaled), np.nan)
        
        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬çš„ç´¢å¼•
        valid_indices = list(range(self.lookback - 1, len(features_scaled)))
        
        print("\nç”Ÿæˆé¢„æµ‹ä¿¡å·ä¸­...")
        
        # æ‰¹å¤„ç†æ¨ç†
        for batch_start in tqdm(range(0, len(valid_indices), batch_size)):
            batch_end = min(batch_start + batch_size, len(valid_indices))
            batch_idx = valid_indices[batch_start:batch_end]
            
            # æ„å»ºæ‰¹æ ·æœ¬
            batch_windows = []
            for i in batch_idx:
                window = features_scaled[i - self.lookback + 1:i + 1]
                batch_windows.append(window)
            
            # è½¬ä¸ºå¼ é‡
            x_batch = torch.FloatTensor(np.array(batch_windows)).to(self.device)
            
            # æ‰¹æ¨ç†
            with torch.no_grad():
                preds = self.model(x_batch)
                pred_values = preds.cpu().numpy().flatten()
            
            # å¡«å……ç»“æœ
            for j, idx in enumerate(batch_idx):
                signals[idx] = pred_values[j]
        
        # æ„é€ è¾“å‡ºDataFrame
        signal_df = pd.DataFrame({
            'ts_event': df['ts_event'],
            'signal': signals
        })
        
        # ä¿ç•™3ä½å°æ•°
        signal_df['signal'] = signal_df['signal'].round(3)
        
        return signal_df


def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    """
    è¯„ä¼°é¢„æµ‹æ•ˆæœ
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    # ç§»é™¤NaN
    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    
    # IC (Information Coefficient)
    ic = np.corrcoef(y_true, y_pred)[0, 1]
    
    # æ–¹å‘å‡†ç¡®ç‡
    direction_true = np.sign(y_true)
    direction_pred = np.sign(y_pred)
    direction_acc = np.mean(direction_true == direction_pred)
    
    # MSE
    mse = np.mean((y_true - y_pred) ** 2)
    
    return {
        'IC': ic,
        'direction_accuracy': direction_acc,
        'MSE': mse
    }


def main(config=None):
    """
    ä¸»å‡½æ•°
    
    Args:
        config: dict, å¯é€‰çš„é…ç½®å­—å…¸ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    """
    if config is None:
        # å‘½ä»¤è¡Œæ¨¡å¼
        parser = argparse.ArgumentParser(
            description='LSTMä»·æ ¼æ–¹å‘é¢„æµ‹å™¨',
            epilog='ç¤ºä¾‹: python lstm_predictor.py --input data.csv --device auto --start 2024-01-01 --end 2024-12-31'
        )
        parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--lookback', type=int, default=60, help='è¾“å…¥çª—å£é•¿åº¦')
        parser.add_argument('--horizon', type=int, default=5, help='é¢„æµ‹æ—¶é—´è·¨åº¦')
        parser.add_argument('--output_dir', type=str, default='./lstm_output', help='è¾“å‡ºç›®å½•')
        parser.add_argument('--model_path', type=str, default=None, help='å·²è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆä»…æ¨ç†æ—¶ä½¿ç”¨ï¼‰')
        parser.add_argument('--inference_only', action='store_true', help='ä»…æ¨ç†æ¨¡å¼')
        parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
        parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
        parser.add_argument('--device', type=str, default='auto', 
                            help='è®¾å¤‡é€‰æ‹©: auto (è‡ªåŠ¨), cuda, mps, cpu. autoä¼šæŒ‰CUDAâ†’MPSâ†’CPUé¡ºåºå°è¯•')
        parser.add_argument('--start', type=str, default=None, help='å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼: YYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºæ•°æ®èµ·å§‹æ—¥æœŸ')
        parser.add_argument('--end', type=str, default=None, help='ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼: YYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºæ•°æ®ç»“æŸæ—¥æœŸ')
        
        args = parser.parse_args()
    else:
        # å­—å…¸é…ç½®æ¨¡å¼ï¼ˆJupyter Notebookï¼‰
        class DictToArgs:
            def __init__(self, config_dict):
                for key, value in config_dict.items():
                    setattr(self, key, value)
        
        # è®¾ç½®é»˜è®¤å€¼
        default_config = {
            'input': None,
            'lookback': 60,
            'horizon': 5,
            'output_dir': './lstm_output',
            'model_path': None,
            'inference_only': False,
            'epochs': 100,
            'batch_size': 64,
            'device': 'auto',
            'start': None,
            'end': None
        }
        default_config.update(config)
        args = DictToArgs(default_config)
        
        if args.input is None:
            raise ValueError("å¿…é¡»æŒ‡å®š 'input' å‚æ•°")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡ - æ™ºèƒ½é€‰æ‹©ï¼šCUDA â†’ MPS â†’ CPU
    if args.device == 'auto':
        # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡
        if torch.cuda.is_available():
            device = 'cuda'
            print(f"âœ“ è‡ªåŠ¨é€‰æ‹©: CUDA GPU åŠ é€Ÿ (æ£€æµ‹åˆ° NVIDIA GPU)")
            # æ˜¾ç¤º CUDA è®¾å¤‡ä¿¡æ¯
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        elif torch.backends.mps.is_available():
            device = 'mps'
            print(f"âœ“ è‡ªåŠ¨é€‰æ‹©: MPS GPU åŠ é€Ÿ (æ£€æµ‹åˆ° Apple Silicon)")
        else:
            device = 'cpu'
            print(f"â„¹ è‡ªåŠ¨é€‰æ‹©: CPU è®¡ç®— (æœªæ£€æµ‹åˆ°å¯ç”¨GPU)")
    elif args.device == 'cuda':
        if torch.cuda.is_available():
            device = 'cuda'
            print(f"âœ“ ä½¿ç”¨ CUDA GPU åŠ é€Ÿ")
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print(f"âš  CUDA ä¸å¯ç”¨ï¼Œå°è¯• MPS...")
            if torch.backends.mps.is_available():
                device = 'mps'
                print(f"âœ“ é™çº§åˆ° MPS GPU")
            else:
                device = 'cpu'
                print(f"âš  MPS ä¹Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
    elif args.device == 'mps':
        if torch.backends.mps.is_available():
            device = 'mps'
            print(f"âœ“ ä½¿ç”¨ MPS GPU åŠ é€Ÿ")
        else:
            print(f"âš  MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
            device = 'cpu'
    else:
        device = 'cpu'
        print(f"â„¹ ä½¿ç”¨ CPU è®¡ç®—")
    
    print(f"\næœ€ç»ˆè®¾å¤‡: {device}")
    if device == 'cpu':
        print(f"CPU çº¿ç¨‹æ•°: {torch.get_num_threads()}")
    
    # ========== 1. åŠ è½½æ•°æ® ==========
    print(f"\nåŠ è½½æ•°æ®: {args.input}")
    df = pd.read_csv(args.input)
    
    # æ£€æŸ¥å¿…éœ€çš„åˆ—
    required_cols = ['ts_event', 'open', 'high', 'low', 'close', 'volume']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {col}")
    
    print(f"åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"åŸå§‹æ—¶é—´èŒƒå›´: {df['ts_event'].iloc[0]} ~ {df['ts_event'].iloc[-1]}")
    
    # ========== æ—¶é—´èŒƒå›´è¿‡æ»¤ ==========
    if args.start is not None or args.end is not None:
        print(f"\nåº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤...")
        # è½¬æ¢ts_eventä¸ºdatetime
        df['ts_event_dt'] = pd.to_datetime(df['ts_event'])
        
        if args.start is not None:
            start_dt = pd.to_datetime(args.start)
            df = df[df['ts_event_dt'] >= start_dt]
            print(f"  èµ·å§‹æ—¶é—´: {args.start}")
        
        if args.end is not None:
            end_dt = pd.to_datetime(args.end)
            df = df[df['ts_event_dt'] <= end_dt]
            print(f"  ç»“æŸæ—¶é—´: {args.end}")
        
        # åˆ é™¤ä¸´æ—¶åˆ—
        df = df.drop(columns=['ts_event_dt'])
        
        # é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        print(f"\nè¿‡æ»¤åæ•°æ®è¡Œæ•°: {len(df)}")
        print(f"è¿‡æ»¤åæ—¶é—´èŒƒå›´: {df['ts_event'].iloc[0]} ~ {df['ts_event'].iloc[-1]}")
        
        if len(df) < args.lookback + args.horizon + 100:
            raise ValueError(f"è¿‡æ»¤åæ•°æ®é‡å¤ªå°‘ï¼ˆ{len(df)}è¡Œï¼‰ï¼Œè‡³å°‘éœ€è¦ {args.lookback + args.horizon + 100} è¡Œ")
    else:
        print(f"\næœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
    
    # ========== 2. ç‰¹å¾å·¥ç¨‹ ==========
    print("\nè®¡ç®—ç‰¹å¾...")
    feature_engine = FeatureEngine()
    features = feature_engine.transform(df)
    print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
    
    # ========== 3. æ„é€ æ ‡ç­¾ ==========
    print("\næ„é€ æ ‡ç­¾...")
    target_engine = TargetEngine(horizon=args.horizon)
    labels = target_engine.create_labels(df)
    print(f"æ ‡ç­¾ç»´åº¦: {labels.shape}")
    
    # ========== 4. æ•°æ®åˆ’åˆ† ==========
    n = len(features)
    train_end = int(n * 0.70)
    val_end = int(n * 0.85)
    
    X_train, y_train = features[:train_end], labels[:train_end]
    X_val, y_val = features[train_end:val_end], labels[train_end:val_end]
    X_test, y_test = features[val_end:], labels[val_end:]
    
    print(f"\næ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # ========== 5. æ ‡å‡†åŒ– ==========
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # ä¿å­˜scaler
    scaler_path = output_dir / f"scaler_L{args.lookback}_H{args.horizon}.pkl"
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"\nScalerå·²ä¿å­˜: {scaler_path}")
    
    # ========== 6. è®­ç»ƒæˆ–åŠ è½½æ¨¡å‹ ==========
    # æ ¹æ®è®¾å¤‡é€‰æ‹© num_workersï¼ˆGPU ä½¿ç”¨ 0ï¼ŒCPU ä½¿ç”¨ 4ï¼‰
    num_workers = 0 if device in ['mps', 'cuda'] else 4
    
    if not args.inference_only:
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TimeSeriesDataset(X_train_scaled, y_train, args.lookback)
        val_dataset = TimeSeriesDataset(X_val_scaled, y_val, args.lookback)
        
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers)
        
        # åˆ›å»ºæ¨¡å‹
        model = LSTMModel(input_size=8, hidden_size=32, num_layers=2, dropout=0.2)
        
        # è®­ç»ƒ
        trainer = Trainer(model, device=device)
        training_log = trainer.train(
            train_loader, val_loader,
            epochs=args.epochs,
            lr=0.001,
            weight_decay=0.0001,
            patience=10
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = output_dir / f"lstm_L{args.lookback}_H{args.horizon}.pt"
        torch.save(model.state_dict(), model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        log_path = output_dir / f"training_log_L{args.lookback}_H{args.horizon}.json"
        with open(log_path, 'w') as f:
            json.dump(training_log, f, indent=2)
        print(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}")
        
        # æµ‹è¯•é›†è¯„ä¼°
        print("\n" + "="*50)
        print("æµ‹è¯•é›†è¯„ä¼°")
        print("="*50)
        
        test_dataset = TimeSeriesDataset(X_test_scaled, y_test, args.lookback)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers)
        
        model.eval()
        y_test_pred = []
        y_test_true = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                outputs = model(batch_x)
                y_test_pred.extend(outputs.cpu().numpy().flatten())
                y_test_true.extend(batch_y.numpy().flatten())
        
        metrics = evaluate_predictions(np.array(y_test_true), np.array(y_test_pred))
        print(f"IC (Information Coefficient): {metrics['IC']:.4f}")
        print(f"æ–¹å‘å‡†ç¡®ç‡: {metrics['direction_accuracy']*100:.1f}%")
        print(f"MSE: {metrics['MSE']:.4f}")
        print("="*50 + "\n")
        
    else:
        # åŠ è½½å·²æœ‰æ¨¡å‹
        if args.model_path is None:
            raise ValueError("æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š--model_path")
        
        model = LSTMModel(input_size=8, hidden_size=32, num_layers=2, dropout=0.2)
        # æ™ºèƒ½åŠ è½½æ¨¡å‹ï¼ˆå¤„ç†è·¨è®¾å¤‡æƒ…å†µï¼‰
        if device == 'cuda':
            model.load_state_dict(torch.load(args.model_path, map_location='cuda'))
        elif device == 'mps':
            model.load_state_dict(torch.load(args.model_path, map_location='cpu'))
        else:
            model.load_state_dict(torch.load(args.model_path, map_location='cpu'))
        print(f"æ¨¡å‹å·²åŠ è½½: {args.model_path}")
    
    # ========== 7. ç”Ÿæˆä¿¡å·CSV ==========
    # æ³¨æ„ï¼šå¯¹æ•´ä¸ªè¿‡æ»¤åçš„æ•°æ®é›†ç”Ÿæˆä¿¡å·
    print(f"\nç”Ÿæˆä¿¡å·ï¼ˆæ•°æ®é‡: {len(df)} è¡Œï¼‰...")
    features_full = feature_engine.transform(df)
    scaler_full = StandardScaler()
    scaler_full.fit(features_full[:train_end])  # åªç”¨è®­ç»ƒé›†fit
    
    predictor = Predictor(model, scaler_full, args.lookback, device=device)
    signal_df = predictor.predict(df, feature_engine, batch_size=args.batch_size)
    
    # ä¿å­˜ä¿¡å·CSV
    input_name = Path(args.input).stem
    output_name = f"{input_name}_signal_L{args.lookback}_H{args.horizon}.csv"
    output_path = output_dir / output_name
    signal_df.to_csv(output_path, index=False)
    
    print(f"\n{'='*50}")
    print(f"ä¿¡å·CSVå·²ç”Ÿæˆ: {output_path}")
    print(f"æ€»è¡Œæ•°: {len(signal_df)}")
    print(f"æœ‰æ•ˆä¿¡å·æ•°: {signal_df['signal'].notna().sum()}")
    print("="*50)
    
    # æ˜¾ç¤ºä¿¡å·ç»Ÿè®¡
    valid_signals = signal_df['signal'].dropna()
    if len(valid_signals) > 0:
        print(f"\nä¿¡å·ç»Ÿè®¡:")
        print(f"  å‡å€¼: {valid_signals.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {valid_signals.std():.4f}")
        print(f"  æœ€å°å€¼: {valid_signals.min():.4f}")
        print(f"  æœ€å¤§å€¼: {valid_signals.max():.4f}")
        print(f"  çœ‹å¤šä¿¡å·(>0.5): {(valid_signals > 0.5).sum()} ({(valid_signals > 0.5).sum()/len(valid_signals)*100:.1f}%)")
        print(f"  çœ‹ç©ºä¿¡å·(<-0.5): {(valid_signals < -0.5).sum()} ({(valid_signals < -0.5).sum()/len(valid_signals)*100:.1f}%)")


if __name__ == '__main__':
    # ====================================================================
    # Jupyter Notebook é…ç½®æ¨¡å¼ï¼ˆå–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨ï¼‰
    # ====================================================================
    # CONFIG = {
    #     'input': 'btc_m1_bars.csv',           # è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
    #     'lookback': 60,                       # è¾“å…¥çª—å£é•¿åº¦
    #     'horizon': 5,                         # é¢„æµ‹æ—¶é—´è·¨åº¦
    #     'output_dir': './lstm_output',        # è¾“å‡ºç›®å½•
    #     'epochs': 100,                        # è®­ç»ƒè½®æ•°
    #     'batch_size': 128,                    # æ‰¹æ¬¡å¤§å°ï¼ˆGPUå¯ä»¥è®¾å¤§ä¸€äº›ï¼‰
    #     'device': 'auto',                     # auto/cuda/mps/cpu
    #     # 'start': '2024-01-01',              # å¯é€‰ï¼šå¼€å§‹æ—¥æœŸ
    #     # 'end': '2024-12-31',                # å¯é€‰ï¼šç»“æŸæ—¥æœŸ
    #     # 'model_path': 'lstm_output/lstm_L60_H5.pt',  # ä»…æ¨ç†æ—¶æŒ‡å®š
    #     # 'inference_only': True,               # ä»…æ¨ç†æ¨¡å¼
    # }
    # ====================================================================
    
    # æ‰“å°è®¾å¤‡ä¿¡æ¯
    print("\n" + "="*60)
    print("LSTM ä»·æ ¼æ–¹å‘é¢„æµ‹å™¨")
    print("="*60)
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"\nå¯ç”¨è®¾å¤‡:")
    print(f"  CUDA: {torch.cuda.is_available()}", end="")
    if torch.cuda.is_available():
        print(f" ({torch.cuda.get_device_name(0)})")
    else:
        print()
    print(f"  MPS:  {torch.backends.mps.is_available()} (å†…ç½®: {torch.backends.mps.is_built()})")
    print(f"  CPU:  {os.cpu_count()} æ ¸å¿ƒ")
    print("="*60 + "\n")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é…ç½®å­—å…¸æ¨¡å¼
    if 'CONFIG' in locals() or 'CONFIG' in globals():
        print("ğŸ“ ä½¿ç”¨é…ç½®å­—å…¸æ¨¡å¼ (Jupyter Notebook)")
        main(config=CONFIG)
    else:
        print("ğŸ“ ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼")
        main()
